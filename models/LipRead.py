import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

import re

from .ConvFrontend import ConvFrontend
from .ResNetBBC import ResNetBBC
from .LSTMBackend import LSTMBackend
from .ConvBackend import ConvBackend
import numpy as np

class LipRead(nn.Module):
    def __init__(self, options):
        super(LipRead, self).__init__()
        self.frontend = ConvFrontend()
        self.resnet = ResNetBBC(options)
        self.backend = ConvBackend(options)
        self.lstm = LSTMBackend(options)

        self.type = "LSTM"#options["model"]["type"]

        def freeze(m):
            m.requires_grad=False

        if(self.type == "LSTM-init"):
            self.frontend.apply(freeze)
            self.resnet.apply(freeze)


        self.frontend.apply(freeze)
        self.resnet.apply(freeze)

        #function to initialize the weights and biases of each module. Matches the
        #classname with a regular expression to determine the type of the module, then
        #initializes the weights for it.
        def weights_init(m):
            classname = m.__class__.__name__
            if re.search("Conv[123]d", classname):
                m.weight.data.normal_(0.0, 0.02)
            elif re.search("BatchNorm[123]d", classname):
                m.weight.data.fill_(1.0)
                m.bias.data.fill_(0)
            elif re.search("Linear", classname):
                m.bias.data.fill_(0)

        #Apply weight initialization to every module in the model.
        self.apply(weights_init)

    def forward(self, input):
        if(self.type == "temp-conv"):
            output = self.backend(self.resnet(self.frontend(input)))

        # print (np.shape (input))
        # print (np.shape (self.frontend(input)))
        # print (np.shape (self.resnet(self.frontend(input))))
        # assert (0)

        if(self.type == "LSTM" or self.type == "LSTM-init"):
            output = self.lstm(self.resnet(self.frontend(input)))

        return output

    def loss(self):
        if(self.type == "temp-conv"):
            return self.backend.loss

        if(self.type == "LSTM" or self.type == "LSTM-init"):
            return self.lstm.loss

    def validator_function(self):
        if(self.type == "temp-conv"):
            return self.backend.validator

        if(self.type == "LSTM" or self.type == "LSTM-init"):
            return self.lstm.validator

    def validator_function_pred(self):
        if(self.type == "temp-conv"):
            return self.backend.validator

        if(self.type == "LSTM" or self.type == "LSTM-init"):
            return self.lstm.validator_pred
